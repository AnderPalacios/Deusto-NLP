{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae3d50c",
   "metadata": {},
   "source": [
    "## **This notebook processes the Cora dataset ready for analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12def7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from processing_Cora import ProcessCora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb96080",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cora_Processing = ProcessCora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ff5bec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique IDs equal to number of Nodes in Cora: True\n",
      "Number of citations equal to number of Edges in Cora: True\n",
      "[35, 40, 114, 117, 128]\n"
     ]
    }
   ],
   "source": [
    "paper_ids = Cora_Processing.get_info_ids(Cora_Processing.ORIGINAL_CORA)\n",
    "print(sorted(paper_ids)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16a6d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{2: ['http:##dimacs.rutgers.edu#techps#1994#94-07.ps']}, {16: ['http:##www.cs.wisc.edu#~fischer#ftp#pub#tech-reports#ncstrl.uwmadison#CS-TR-90-907#CS-TR-90-907.ps.Z']}]\n"
     ]
    }
   ],
   "source": [
    "# Diferent files named as urls contain information about each paper (e.g., Title, Abstract)\n",
    "ids_to_filenames = Cora_Processing.get_filenames(Cora_Processing.FILENAMES_PATH)\n",
    "print(ids_to_filenames[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4931e791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal number of ids and nodes -> True\n",
      "Same ids as paper Ids -> True\n",
      "[{35: ['http:##www.cis.ohio-state.edu#lair#TechReports#93-pa-ep93.ps', 'http:##www.cs.purdue.edu#coast#archive#clife#GA#docs#tcga91-2.ps.gz']}, {40: ['http:##www.bioele.nuee.nagoya-u.ac.jp#wec#papers#files#lee.ps.gz']}]\n"
     ]
    }
   ],
   "source": [
    "# Get only Cora papers\n",
    "cora_ids = Cora_Processing.get_original_Cora(paper_ids, ids_to_filenames)\n",
    "print(cora_ids[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b9f134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{35: ['http_##www.cis.ohio-state.edu#lair#TechReports#93-pa-ep93.ps', 'http_##www.cs.purdue.edu#coast#archive#clife#GA#docs#tcga91-2.ps.gz']}, {40: ['http_##www.bioele.nuee.nagoya-u.ac.jp#wec#papers#files#lee.ps.gz']}]\n",
      "Papers with info: 2707\n",
      "Papers missing info: 1\n",
      "Total papers: 2708\n"
     ]
    }
   ],
   "source": [
    "# Fix the urls to associate them (replace ':' with '_' in most of the cases)\n",
    "fixed_protocols = Cora_Processing.update_protocol(cora_ids)\n",
    "print(fixed_protocols[:2])\n",
    "\n",
    "# Update dataset after looking missing files with info\n",
    "new_dataset = Cora_Processing.check_missing_data(fixed_protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a33e8a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 {'Title': 'Evolutionary Module Acquisition (1993) Coevolving high-level representations, Artificial Life (1989) Genetic Algorithms in Search, Optimization,', 'Author': 'Angeline and Pollack Angeline, P. and Pollack, J. III, Fogel, L., Owens, A., and Walsh, M. Goldberg, D. Davis editor, Morgan Kaufman. Holland, J. Jefferson, D., R. Collins, C. Cooper, M. Dyer, M. Flowers, R. Korf, C. Taylor, and A. Wang. edited by C. Langton, C. Taylor, J. Farmer and S.', 'Abstract': 'Angeline, P., Saunders, G. and Pollack, J. (1993) An evolutionary algorithm that constructs recurrent neural networks, LAIR Technical Report #93-PA-GNARLY, Submitted to IEEE Transactions on Neural Networks Special Issue on Evolutionary Programming.'}\n",
      "40 {'Title': 'Dynamic Control of Genetic Algorithms using Fuzzy Logic Techniques', 'Author': 'Michael A. LEE Hideyuki TAKAGI', 'Abstract': 'This paper proposes using fuzzy logic techniques to dynamically control parameter settings of genetic algorithms (GAs). We describe the Dynamic Parametric GA: a GA that uses a fuzzy knowledge-based system to control GA parameters. We then introduce a technique for automatically designing and tuning the fuzzy knowledge-base system using GAs. Results from initial experiments show a performance improvement over a simple static GA. One Dynamic Parametric GA system designed by our automatic method demonstrated improvement on an application not included in the design phase, which may indicate the general applicability of the Dynamic Parametric GA to a wide range of ap plications.'}\n",
      "114 {'Title': 'Learning to Act using Real-Time Dynamic Programming', 'Author': 'Andrew G. Barto Steven J. Bradtke Satinder P. Singh', 'Abstract': \"fl The authors thank Rich Yee, Vijay Gullapalli, Brian Pinette, and Jonathan Bachrach for helping to clarify the relationships between heuristic search and control. We thank Rich Sutton, Chris Watkins, Paul Werbos, and Ron Williams for sharing their fundamental insights into this subject through numerous discussions, and we further thank Rich Sutton for first making us aware of Korf's research and for his very thoughtful comments on the manuscript. We are very grateful to Dimitri Bertsekas and Steven Sullivan for independently pointing out an error in an earlier version of this article. Finally, we thank Harry Klopf, whose insight and persistence encouraged our interest in this class of learning problems. This research was supported by grants to A.G. Barto from the National Science Foundation (ECS-8912623 and ECS-9214866) and the Air Force Office of Scientific Research, Bolling AFB (AFOSR-89-0526).\"}\n",
      "117 {'Title': 'Generalized Markov Decision Processes: Dynamic-programming and Reinforcement-learning Algorithms', 'Author': 'Csaba Szepesvari Michael L. Littman', 'Abstract': 'The problem of maximizing the expected total discounted reward in a completely observable Markovian environment, i.e., a Markov decision process (mdp), models a particular class of sequential decision problems. Algorithms have been developed for making optimal decisions in mdps given either an mdp specification or the opportunity to interact with the mdp over time. Recently, other sequential decision-making problems have been studied prompting the development of new algorithms and analyses. We describe a new generalized model that subsumes mdps as well as many of the recent variations. We prove some basic results concerning this model and develop generalizations of value iteration, policy iteration, model-based reinforcement-learning, and Q-learning that can be used to make optimal decisions in the generalized model under various assumptions. Applications of the theory to particular models are described, including risk-averse mdps, exploration-sensitive mdps, sarsa, Q-learning with spreading, two-player games, and approximate max picking via sampling. Central to the results are the contraction property of the value operator and a stochastic-approximation theorem that reduces asynchronous convergence to synchronous convergence.'}\n",
      "128 {'Title': 'Reinforcement Learning Algorithms for Average-Payoff Markovian Decision Processes', 'Author': 'Satinder P. Singh', 'Abstract': \"Reinforcement learning (RL) has become a central paradigm for solving learning-control problems in robotics and artificial intelligence. RL researchers have focussed almost exclusively on problems where the controller has to maximize the discounted sum of payoffs. However, as emphasized by Schwartz (1993), in many problems, e.g., those for which the optimal behavior is a limit cycle, it is more natural and com-putationally advantageous to formulate tasks so that the controller's objective is to maximize the average payoff received per time step. In this paper I derive new average-payoff RL algorithms as stochastic approximation methods for solving the system of equations associated with the policy evaluation and optimal control questions in average-payoff RL tasks. These algorithms are analogous to the popular TD and Q-learning algorithms already developed for the discounted-payoff case. One of the algorithms derived here is a significant variation of Schwartz's R-learning algorithm. Preliminary empirical results are presented to validate these new algorithms.\"}\n",
      "\n",
      "Number of records: 2707\n",
      "{'paper_id': 35, 'filenames': ['http_##www.cis.ohio-state.edu#lair#TechReports#93-pa-ep93.ps', 'http_##www.cs.purdue.edu#coast#archive#clife#GA#docs#tcga91-2.ps.gz']}\n",
      "{'paper_id': 40, 'filenames': ['http_##www.bioele.nuee.nagoya-u.ac.jp#wec#papers#files#lee.ps.gz']}\n",
      "{'paper_id': 114, 'filenames': ['ftp_##ftp.cs.colorado.edu#users#baveja#Papers#realtime-dp.ps.gz', 'ftp_##ftp.cs.umass.edu#pub#anw#pub#barto#realtime-dp.ps.Z', 'ftp_##ftp.cs.umass.edu#pub#techrept#techreport#1993#UM-CS-1993-002.ps', 'http_##www-anw.cs.umass.edu#People#singh#Papers#realtime-dp.ps']}\n",
      "{'paper_id': 117, 'filenames': ['http_##www.cs.duke.edu#~mlittman#docs#gmdp.ps']}\n",
      "{'paper_id': 128, 'filenames': ['ftp_##ftp.cs.colorado.edu#users#baveja#Papers#AAAI94.ps.gz', 'http_##www-anw.cs.umass.edu#cgi-bin#getfile#pub#anw#pub#singh#singh-AAAI94.ps.Z']}\n",
      "\n",
      "Number of records 2409 -> Few papers did not have info about theur Title or Abstract so they are useless\n",
      "{'paper_id': 35, 'Title': 'Evolutionary Module Acquisition (1993) Coevolving high-level representations, Artificial Life (1989) Genetic Algorithms in Search, Optimization,', 'Author': 'Angeline and Pollack Angeline, P. and Pollack, J. III, Fogel, L., Owens, A., and Walsh, M. Goldberg, D. Davis editor, Morgan Kaufman. Holland, J. Jefferson, D., R. Collins, C. Cooper, M. Dyer, M. Flowers, R. Korf, C. Taylor, and A. Wang. edited by C. Langton, C. Taylor, J. Farmer and S.', 'Abstract': 'Angeline, P., Saunders, G. and Pollack, J. (1993) An evolutionary algorithm that constructs recurrent neural networks, LAIR Technical Report #93-PA-GNARLY, Submitted to IEEE Transactions on Neural Networks Special Issue on Evolutionary Programming.'}\n",
      "{'paper_id': 40, 'Title': 'Dynamic Control of Genetic Algorithms using Fuzzy Logic Techniques', 'Author': 'Michael A. LEE Hideyuki TAKAGI', 'Abstract': 'This paper proposes using fuzzy logic techniques to dynamically control parameter settings of genetic algorithms (GAs). We describe the Dynamic Parametric GA: a GA that uses a fuzzy knowledge-based system to control GA parameters. We then introduce a technique for automatically designing and tuning the fuzzy knowledge-base system using GAs. Results from initial experiments show a performance improvement over a simple static GA. One Dynamic Parametric GA system designed by our automatic method demonstrated improvement on an application not included in the design phase, which may indicate the general applicability of the Dynamic Parametric GA to a wide range of ap plications.'}\n",
      "{'paper_id': 114, 'Title': 'Learning to Act using Real-Time Dynamic Programming', 'Author': 'Andrew G. Barto Steven J. Bradtke Satinder P. Singh', 'Abstract': \"fl The authors thank Rich Yee, Vijay Gullapalli, Brian Pinette, and Jonathan Bachrach for helping to clarify the relationships between heuristic search and control. We thank Rich Sutton, Chris Watkins, Paul Werbos, and Ron Williams for sharing their fundamental insights into this subject through numerous discussions, and we further thank Rich Sutton for first making us aware of Korf's research and for his very thoughtful comments on the manuscript. We are very grateful to Dimitri Bertsekas and Steven Sullivan for independently pointing out an error in an earlier version of this article. Finally, we thank Harry Klopf, whose insight and persistence encouraged our interest in this class of learning problems. This research was supported by grants to A.G. Barto from the National Science Foundation (ECS-8912623 and ECS-9214866) and the Air Force Office of Scientific Research, Bolling AFB (AFOSR-89-0526).\"}\n",
      "{'paper_id': 117, 'Title': 'Generalized Markov Decision Processes: Dynamic-programming and Reinforcement-learning Algorithms', 'Author': 'Csaba Szepesvari Michael L. Littman', 'Abstract': 'The problem of maximizing the expected total discounted reward in a completely observable Markovian environment, i.e., a Markov decision process (mdp), models a particular class of sequential decision problems. Algorithms have been developed for making optimal decisions in mdps given either an mdp specification or the opportunity to interact with the mdp over time. Recently, other sequential decision-making problems have been studied prompting the development of new algorithms and analyses. We describe a new generalized model that subsumes mdps as well as many of the recent variations. We prove some basic results concerning this model and develop generalizations of value iteration, policy iteration, model-based reinforcement-learning, and Q-learning that can be used to make optimal decisions in the generalized model under various assumptions. Applications of the theory to particular models are described, including risk-averse mdps, exploration-sensitive mdps, sarsa, Q-learning with spreading, two-player games, and approximate max picking via sampling. Central to the results are the contraction property of the value operator and a stochastic-approximation theorem that reduces asynchronous convergence to synchronous convergence.'}\n",
      "{'paper_id': 128, 'Title': 'Reinforcement Learning Algorithms for Average-Payoff Markovian Decision Processes', 'Author': 'Satinder P. Singh', 'Abstract': \"Reinforcement learning (RL) has become a central paradigm for solving learning-control problems in robotics and artificial intelligence. RL researchers have focussed almost exclusively on problems where the controller has to maximize the discounted sum of payoffs. However, as emphasized by Schwartz (1993), in many problems, e.g., those for which the optimal behavior is a limit cycle, it is more natural and com-putationally advantageous to formulate tasks so that the controller's objective is to maximize the average payoff received per time step. In this paper I derive new average-payoff RL algorithms as stochastic approximation methods for solving the system of equations associated with the policy evaluation and optimal control questions in average-payoff RL tasks. These algorithms are analogous to the popular TD and Q-learning algorithms already developed for the discounted-payoff case. One of the algorithms derived here is a significant variation of Schwartz's R-learning algorithm. Preliminary empirical results are presented to validate these new algorithms.\"}\n",
      "\n",
      "Merged DataFrame; number of records: 2409\n",
      "{'paper_id': 35, 'filenames': ['http_##www.cis.ohio-state.edu#lair#TechReports#93-pa-ep93.ps', 'http_##www.cs.purdue.edu#coast#archive#clife#GA#docs#tcga91-2.ps.gz'], 'Title': 'Evolutionary Module Acquisition (1993) Coevolving high-level representations, Artificial Life (1989) Genetic Algorithms in Search, Optimization,', 'Author': 'Angeline and Pollack Angeline, P. and Pollack, J. III, Fogel, L., Owens, A., and Walsh, M. Goldberg, D. Davis editor, Morgan Kaufman. Holland, J. Jefferson, D., R. Collins, C. Cooper, M. Dyer, M. Flowers, R. Korf, C. Taylor, and A. Wang. edited by C. Langton, C. Taylor, J. Farmer and S.', 'Abstract': 'Angeline, P., Saunders, G. and Pollack, J. (1993) An evolutionary algorithm that constructs recurrent neural networks, LAIR Technical Report #93-PA-GNARLY, Submitted to IEEE Transactions on Neural Networks Special Issue on Evolutionary Programming.'}\n",
      "{'paper_id': 40, 'filenames': ['http_##www.bioele.nuee.nagoya-u.ac.jp#wec#papers#files#lee.ps.gz'], 'Title': 'Dynamic Control of Genetic Algorithms using Fuzzy Logic Techniques', 'Author': 'Michael A. LEE Hideyuki TAKAGI', 'Abstract': 'This paper proposes using fuzzy logic techniques to dynamically control parameter settings of genetic algorithms (GAs). We describe the Dynamic Parametric GA: a GA that uses a fuzzy knowledge-based system to control GA parameters. We then introduce a technique for automatically designing and tuning the fuzzy knowledge-base system using GAs. Results from initial experiments show a performance improvement over a simple static GA. One Dynamic Parametric GA system designed by our automatic method demonstrated improvement on an application not included in the design phase, which may indicate the general applicability of the Dynamic Parametric GA to a wide range of ap plications.'}\n",
      "{'paper_id': 114, 'filenames': ['ftp_##ftp.cs.colorado.edu#users#baveja#Papers#realtime-dp.ps.gz', 'ftp_##ftp.cs.umass.edu#pub#anw#pub#barto#realtime-dp.ps.Z', 'ftp_##ftp.cs.umass.edu#pub#techrept#techreport#1993#UM-CS-1993-002.ps', 'http_##www-anw.cs.umass.edu#People#singh#Papers#realtime-dp.ps'], 'Title': 'Learning to Act using Real-Time Dynamic Programming', 'Author': 'Andrew G. Barto Steven J. Bradtke Satinder P. Singh', 'Abstract': \"fl The authors thank Rich Yee, Vijay Gullapalli, Brian Pinette, and Jonathan Bachrach for helping to clarify the relationships between heuristic search and control. We thank Rich Sutton, Chris Watkins, Paul Werbos, and Ron Williams for sharing their fundamental insights into this subject through numerous discussions, and we further thank Rich Sutton for first making us aware of Korf's research and for his very thoughtful comments on the manuscript. We are very grateful to Dimitri Bertsekas and Steven Sullivan for independently pointing out an error in an earlier version of this article. Finally, we thank Harry Klopf, whose insight and persistence encouraged our interest in this class of learning problems. This research was supported by grants to A.G. Barto from the National Science Foundation (ECS-8912623 and ECS-9214866) and the Air Force Office of Scientific Research, Bolling AFB (AFOSR-89-0526).\"}\n",
      "{'paper_id': 117, 'filenames': ['http_##www.cs.duke.edu#~mlittman#docs#gmdp.ps'], 'Title': 'Generalized Markov Decision Processes: Dynamic-programming and Reinforcement-learning Algorithms', 'Author': 'Csaba Szepesvari Michael L. Littman', 'Abstract': 'The problem of maximizing the expected total discounted reward in a completely observable Markovian environment, i.e., a Markov decision process (mdp), models a particular class of sequential decision problems. Algorithms have been developed for making optimal decisions in mdps given either an mdp specification or the opportunity to interact with the mdp over time. Recently, other sequential decision-making problems have been studied prompting the development of new algorithms and analyses. We describe a new generalized model that subsumes mdps as well as many of the recent variations. We prove some basic results concerning this model and develop generalizations of value iteration, policy iteration, model-based reinforcement-learning, and Q-learning that can be used to make optimal decisions in the generalized model under various assumptions. Applications of the theory to particular models are described, including risk-averse mdps, exploration-sensitive mdps, sarsa, Q-learning with spreading, two-player games, and approximate max picking via sampling. Central to the results are the contraction property of the value operator and a stochastic-approximation theorem that reduces asynchronous convergence to synchronous convergence.'}\n",
      "{'paper_id': 128, 'filenames': ['ftp_##ftp.cs.colorado.edu#users#baveja#Papers#AAAI94.ps.gz', 'http_##www-anw.cs.umass.edu#cgi-bin#getfile#pub#anw#pub#singh#singh-AAAI94.ps.Z'], 'Title': 'Reinforcement Learning Algorithms for Average-Payoff Markovian Decision Processes', 'Author': 'Satinder P. Singh', 'Abstract': \"Reinforcement learning (RL) has become a central paradigm for solving learning-control problems in robotics and artificial intelligence. RL researchers have focussed almost exclusively on problems where the controller has to maximize the discounted sum of payoffs. However, as emphasized by Schwartz (1993), in many problems, e.g., those for which the optimal behavior is a limit cycle, it is more natural and com-putationally advantageous to formulate tasks so that the controller's objective is to maximize the average payoff received per time step. In this paper I derive new average-payoff RL algorithms as stochastic approximation methods for solving the system of equations associated with the policy evaluation and optimal control questions in average-payoff RL tasks. These algorithms are analogous to the popular TD and Q-learning algorithms already developed for the discounted-payoff case. One of the algorithms derived here is a significant variation of Schwartz's R-learning algorithm. Preliminary empirical results are presented to validate these new algorithms.\"}\n",
      "\n",
      "Number of missing abstract: 0\n",
      "Number of missing abstract: 0\n"
     ]
    }
   ],
   "source": [
    "extractions = Cora_Processing.ALL_EXTRACTIONS\n",
    "paper_info = Cora_Processing.extract_paper_info2(new_dataset, extractions) # ID, Title, Authors and Abstract \n",
    "\n",
    "for i, (paper_id, info) in enumerate(paper_info.items()):\n",
    "    print(paper_id, info)\n",
    "    if i >= 4:  \n",
    "        break\n",
    "\n",
    "# Ids and url to Dataframe\n",
    "ids_files_df = Cora_Processing.list_to_df(new_dataset)\n",
    "print(f\"\\nNumber of records: {len(ids_files_df)}\")\n",
    "first_5_ids_files = ids_files_df.iloc[:5].to_dict(orient='records')\n",
    "for info in first_5_ids_files:\n",
    "    print(info)\n",
    "\n",
    "# Paper info to DataFrame\n",
    "df_info = Cora_Processing.dict_to_df(paper_info)\n",
    "print(f\"\\nNumber of records {len(df_info)} -> Few papers did not have info about theur Title or Abstract so they are useless\")\n",
    "first_5_papers_info = df_info.iloc[:5].to_dict(orient='records')\n",
    "for info in first_5_papers_info:\n",
    "    print(info)\n",
    "\n",
    "\n",
    "\n",
    "# Merge both DataFrames:\n",
    "merged_df = Cora_Processing.merge_dfs(ids_files_df, df_info)\n",
    "print(f\"\\nMerged DataFrame; number of records: {len(merged_df)}\")\n",
    "first_5_merged = merged_df.iloc[:5].to_dict(orient='records')\n",
    "for row in first_5_merged:\n",
    "    print(row)\n",
    "\n",
    "\n",
    "# Sanity check:\n",
    "print()\n",
    "Cora_Processing.check_NA(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7df20b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 -> Genetic_Algorithms\n",
      "40 -> Genetic_Algorithms\n",
      "114 -> Reinforcement_Learning\n",
      "117 -> Reinforcement_Learning\n",
      "128 -> Reinforcement_Learning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>BoW</th>\n",
       "      <th>topic2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31336</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1061127</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>Rule_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1106406</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13195</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37879</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                                BoW  \\\n",
       "0     31336  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1   1061127  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "2   1106406  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     13195  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     37879  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                   topic2  \n",
       "0         Neural_Networks  \n",
       "1           Rule_Learning  \n",
       "2  Reinforcement_Learning  \n",
       "3  Reinforcement_Learning  \n",
       "4   Probabilistic_Methods  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get paper categories:\n",
    "labels_df = Cora_Processing.get_labels(Cora_Processing.CLASS_PATH, merged_df)\n",
    "first_5_labels = labels_df.iloc[:5].to_dict(orient='records')\n",
    "for row in first_5_labels:\n",
    "    print(row['paper_id'],\"->\",row['topic'])\n",
    "\n",
    "# Get Bag of Words (BoW) representation:\n",
    "BoW_df = Cora_Processing.get_Bow(Cora_Processing.BOW_CORA)\n",
    "BoW_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c438f8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>filenames</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>BoW</th>\n",
       "      <th>topic2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>[http_##www.cis.ohio-state.edu#lair#TechReport...</td>\n",
       "      <td>Evolutionary Module Acquisition (1993) Coevolv...</td>\n",
       "      <td>Angeline and Pollack Angeline, P. and Pollack,...</td>\n",
       "      <td>Angeline, P., Saunders, G. and Pollack, J. (19...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Genetic_Algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>[http_##www.bioele.nuee.nagoya-u.ac.jp#wec#pap...</td>\n",
       "      <td>Dynamic Control of Genetic Algorithms using Fu...</td>\n",
       "      <td>Michael A. LEE Hideyuki TAKAGI</td>\n",
       "      <td>This paper proposes using fuzzy logic techniqu...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Genetic_Algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>[ftp_##ftp.cs.colorado.edu#users#baveja#Papers...</td>\n",
       "      <td>Learning to Act using Real-Time Dynamic Progra...</td>\n",
       "      <td>Andrew G. Barto Steven J. Bradtke Satinder P. ...</td>\n",
       "      <td>fl The authors thank Rich Yee, Vijay Gullapall...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117</td>\n",
       "      <td>[http_##www.cs.duke.edu#~mlittman#docs#gmdp.ps]</td>\n",
       "      <td>Generalized Markov Decision Processes: Dynamic...</td>\n",
       "      <td>Csaba Szepesvari Michael L. Littman</td>\n",
       "      <td>The problem of maximizing the expected total d...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128</td>\n",
       "      <td>[ftp_##ftp.cs.colorado.edu#users#baveja#Papers...</td>\n",
       "      <td>Reinforcement Learning Algorithms for Average-...</td>\n",
       "      <td>Satinder P. Singh</td>\n",
       "      <td>Reinforcement learning (RL) has become a centr...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>130</td>\n",
       "      <td>[http_##www.cs.orst.edu#~tadepall#research#pap...</td>\n",
       "      <td>Scaling Up Average Reward Reinforcement Learni...</td>\n",
       "      <td>Prasad Tadepalli and DoKyeong Ok</td>\n",
       "      <td>Almost all the work in Average-reward Reinforc...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>164</td>\n",
       "      <td>[ftp_##theory.lcs.mit.edu#pub#people#oded#grs....</td>\n",
       "      <td>Learning polynomials with queries: The highly ...</td>\n",
       "      <td>ODED GOLDREICH RONITT RUBINFELD MADHU SUDAN</td>\n",
       "      <td>Given a function f mapping n-variate inputs fr...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>288</td>\n",
       "      <td>[http_##www.cs.cmu.edu#afs#cs.cmu.edu#user#awd...</td>\n",
       "      <td>Memory Based Stochastic Optimization for Valid...</td>\n",
       "      <td>Artur Dubrawski and Jeff Schneider</td>\n",
       "      <td>This paper focuses on the optimization of hype...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>424</td>\n",
       "      <td>[ftp_##ftp.cs.orst.edu#pub#tgd#papers#mlj-nge....</td>\n",
       "      <td>An Experimental Comparison of the Nearest-Neig...</td>\n",
       "      <td>DIETRICH WETTSCHERECK THOMAS G. DIETTERICH Edi...</td>\n",
       "      <td>Algorithms based on Nested Generalized Exempla...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Rule_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>434</td>\n",
       "      <td>[http_##www.cs.cmu.edu#afs#cs.cmu.edu#Web#Peop...</td>\n",
       "      <td>Learning Analytically and Inductively</td>\n",
       "      <td>Tom M. Mitchell Sebastian B. Thrun</td>\n",
       "      <td>Learning is a fundamental component of intelli...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                          filenames  \\\n",
       "0        35  [http_##www.cis.ohio-state.edu#lair#TechReport...   \n",
       "1        40  [http_##www.bioele.nuee.nagoya-u.ac.jp#wec#pap...   \n",
       "2       114  [ftp_##ftp.cs.colorado.edu#users#baveja#Papers...   \n",
       "3       117    [http_##www.cs.duke.edu#~mlittman#docs#gmdp.ps]   \n",
       "4       128  [ftp_##ftp.cs.colorado.edu#users#baveja#Papers...   \n",
       "5       130  [http_##www.cs.orst.edu#~tadepall#research#pap...   \n",
       "6       164  [ftp_##theory.lcs.mit.edu#pub#people#oded#grs....   \n",
       "7       288  [http_##www.cs.cmu.edu#afs#cs.cmu.edu#user#awd...   \n",
       "8       424  [ftp_##ftp.cs.orst.edu#pub#tgd#papers#mlj-nge....   \n",
       "9       434  [http_##www.cs.cmu.edu#afs#cs.cmu.edu#Web#Peop...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Evolutionary Module Acquisition (1993) Coevolv...   \n",
       "1  Dynamic Control of Genetic Algorithms using Fu...   \n",
       "2  Learning to Act using Real-Time Dynamic Progra...   \n",
       "3  Generalized Markov Decision Processes: Dynamic...   \n",
       "4  Reinforcement Learning Algorithms for Average-...   \n",
       "5  Scaling Up Average Reward Reinforcement Learni...   \n",
       "6  Learning polynomials with queries: The highly ...   \n",
       "7  Memory Based Stochastic Optimization for Valid...   \n",
       "8  An Experimental Comparison of the Nearest-Neig...   \n",
       "9              Learning Analytically and Inductively   \n",
       "\n",
       "                                              Author  \\\n",
       "0  Angeline and Pollack Angeline, P. and Pollack,...   \n",
       "1                     Michael A. LEE Hideyuki TAKAGI   \n",
       "2  Andrew G. Barto Steven J. Bradtke Satinder P. ...   \n",
       "3                Csaba Szepesvari Michael L. Littman   \n",
       "4                                  Satinder P. Singh   \n",
       "5                   Prasad Tadepalli and DoKyeong Ok   \n",
       "6        ODED GOLDREICH RONITT RUBINFELD MADHU SUDAN   \n",
       "7                 Artur Dubrawski and Jeff Schneider   \n",
       "8  DIETRICH WETTSCHERECK THOMAS G. DIETTERICH Edi...   \n",
       "9                 Tom M. Mitchell Sebastian B. Thrun   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Angeline, P., Saunders, G. and Pollack, J. (19...   \n",
       "1  This paper proposes using fuzzy logic techniqu...   \n",
       "2  fl The authors thank Rich Yee, Vijay Gullapall...   \n",
       "3  The problem of maximizing the expected total d...   \n",
       "4  Reinforcement learning (RL) has become a centr...   \n",
       "5  Almost all the work in Average-reward Reinforc...   \n",
       "6  Given a function f mapping n-variate inputs fr...   \n",
       "7  This paper focuses on the optimization of hype...   \n",
       "8  Algorithms based on Nested Generalized Exempla...   \n",
       "9  Learning is a fundamental component of intelli...   \n",
       "\n",
       "                                                 BoW                  topic2  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      Genetic_Algorithms  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      Genetic_Algorithms  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  Reinforcement_Learning  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  Reinforcement_Learning  \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  Reinforcement_Learning  \n",
       "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  Reinforcement_Learning  \n",
       "6  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...                  Theory  \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  Reinforcement_Learning  \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           Rule_Learning  \n",
       "9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  Reinforcement_Learning  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge for the finalized DataFrame\n",
    "final_df = Cora_Processing.merge_info_bow(labels_df, BoW_df)\n",
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb02043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the dataframe as a .csv\n",
    "Cora_Processing.to_csv(final_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
