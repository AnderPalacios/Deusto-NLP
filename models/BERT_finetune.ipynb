{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbbfff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq adapters datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6b177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.51.3\n",
      "Uninstalling transformers-4.51.3:\n",
      "  Successfully uninstalled transformers-4.51.3\n",
      "Found existing installation: adapters 1.2.0\n",
      "Uninstalling adapters-1.2.0:\n",
      "  Successfully uninstalled adapters-1.2.0\n",
      "Found existing installation: accelerate 0.26.0\n",
      "Uninstalling accelerate-0.26.0:\n",
      "  Successfully uninstalled accelerate-0.26.0\n",
      "Found existing installation: tokenizers 0.21.4\n",
      "Uninstalling tokenizers-0.21.4:\n",
      "  Successfully uninstalled tokenizers-0.21.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping peft as it is not installed.\n",
      "WARNING: Skipping trl as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.51.3\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.51.3)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\apala\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers==4.51.3) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers==4.51.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers==4.51.3) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers==4.51.3) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers==4.51.3) (2024.6.2)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   ---------------------------------------- 2/2 [transformers]\n",
      "\n",
      "Successfully installed tokenizers-0.21.4 transformers-4.51.3\n",
      "Collecting adapters==1.2.0\n",
      "  Using cached adapters-1.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: transformers~=4.51.3 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from adapters==1.2.0) (4.51.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from adapters==1.2.0) (23.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers~=4.51.3->adapters==1.2.0) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers~=4.51.3->adapters==1.2.0) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\apala\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers~=4.51.3->adapters==1.2.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers~=4.51.3->adapters==1.2.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers~=4.51.3->adapters==1.2.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers~=4.51.3->adapters==1.2.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers~=4.51.3->adapters==1.2.0) (2024.6.2)\n",
      "Using cached adapters-1.2.0-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: adapters\n",
      "Successfully installed adapters-1.2.0\n",
      "Collecting accelerate==0.26.0\n",
      "  Using cached accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate==0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate==0.26.0) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\apala\\appdata\\roaming\\python\\python312\\site-packages (from accelerate==0.26.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate==0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate==0.26.0) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate==0.26.0) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate==0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (4.15.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (75.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub->accelerate==0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub->accelerate==0.26.0) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\apala\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate==0.26.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate==0.26.0) (1.3.0)\n",
      "Using cached accelerate-0.26.0-py3-none-any.whl (270 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y transformers adapters peft trl accelerate tokenizers\n",
    "!pip install transformers==4.51.3\n",
    "!pip install adapters==1.2.0\n",
    "!pip install accelerate==0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7416e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import RobertaTokenizer, RobertaConfig, TrainingArguments, EvalPrediction\n",
    "from adapters import AutoAdapterModel, AdapterTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea47d76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 8530, 'validation': 1066, 'test': 1066}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tom = load_dataset(\"rotten_tomatoes\")\n",
    "data_tom.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80755c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n",
      "{'text': 'effective but too-tepid biopic', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(data_tom)\n",
    "print(data_tom[\"train\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ad8f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = pd.read_csv(\"../Hyperedges/Cora_dataset.csv\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "cora['labels'] = le.fit_transform(cora['topic2']) \n",
    "\n",
    "cora_ft = Dataset.from_pandas(cora[[\"Abstract\", \"labels\"]])\n",
    "cora_ft = cora_ft.rename_column(\"Abstract\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62b922e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>filenames</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>BoW</th>\n",
       "      <th>topic2</th>\n",
       "      <th>Abstract_W2V</th>\n",
       "      <th>bert_embedding</th>\n",
       "      <th>cluster_embs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>['http_##www.cis.ohio-state.edu#lair#TechRepor...</td>\n",
       "      <td>Evolutionary Module Acquisition (1993) Coevolv...</td>\n",
       "      <td>Angeline and Pollack Angeline, P. and Pollack,...</td>\n",
       "      <td>Angeline, P., Saunders, G. and Pollack, J. (19...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Genetic_Algorithms</td>\n",
       "      <td>[ 0.00257568  0.01624146 -0.01265259  0.129727...</td>\n",
       "      <td>[ 6.13429993e-02 -3.51591222e-02 -8.50764588e-...</td>\n",
       "      <td>[ 2.61604437e-03  2.69720778e-02 -3.40913646e-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>['http_##www.bioele.nuee.nagoya-u.ac.jp#wec#pa...</td>\n",
       "      <td>Dynamic Control of Genetic Algorithms using Fu...</td>\n",
       "      <td>Michael A. LEE Hideyuki TAKAGI</td>\n",
       "      <td>This paper proposes using fuzzy logic techniqu...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Genetic_Algorithms</td>\n",
       "      <td>[ 2.27050781e-02  4.92503271e-02  5.16006462e-...</td>\n",
       "      <td>[ 6.13822490e-02 -9.98590514e-03 -7.81871602e-...</td>\n",
       "      <td>[ 1.28001701e-02  3.91861163e-02 -1.01053238e-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>['ftp_##ftp.cs.colorado.edu#users#baveja#Paper...</td>\n",
       "      <td>Learning to Act using Real-Time Dynamic Progra...</td>\n",
       "      <td>Andrew G. Barto Steven J. Bradtke Satinder P. ...</td>\n",
       "      <td>fl The authors thank Rich Yee, Vijay Gullapall...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "      <td>[-0.01200994  0.03754203  0.00502043  0.093253...</td>\n",
       "      <td>[ 5.97590730e-02 -6.56006113e-03 -7.85881057e-...</td>\n",
       "      <td>[ 2.11583525e-02 -5.75699378e-03 -1.89218428e-...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117</td>\n",
       "      <td>['http_##www.cs.duke.edu#~mlittman#docs#gmdp.ps']</td>\n",
       "      <td>Generalized Markov Decision Processes: Dynamic...</td>\n",
       "      <td>Csaba Szepesvari Michael L. Littman</td>\n",
       "      <td>The problem of maximizing the expected total d...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "      <td>[ 0.01002187  0.00597592  0.02532374  0.099285...</td>\n",
       "      <td>[ 6.34309798e-02 -2.35317685e-02 -8.46546218e-...</td>\n",
       "      <td>[ 1.14918491e-02  3.67332175e-02  7.56769720e-...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128</td>\n",
       "      <td>['ftp_##ftp.cs.colorado.edu#users#baveja#Paper...</td>\n",
       "      <td>Reinforcement Learning Algorithms for Average-...</td>\n",
       "      <td>Satinder P. Singh</td>\n",
       "      <td>Reinforcement learning (RL) has become a centr...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "      <td>[ 0.01000858 -0.0008307   0.03161937  0.086836...</td>\n",
       "      <td>[ 7.14411512e-02 -2.84189135e-02 -8.62762332e-...</td>\n",
       "      <td>[ 7.57048698e-03  3.90252322e-02  5.19991573e-...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>1154276</td>\n",
       "      <td>['ftp_##ftp.icsi.berkeley.edu#pub#techreports#...</td>\n",
       "      <td>Scatter-partitioning RBF network for function ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>segmentation: Preliminary results Abstract. Sc...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "      <td>[-0.00997262  0.04042348  0.02706561  0.063081...</td>\n",
       "      <td>[ 4.68115099e-02 -3.41062658e-02 -8.05736557e-...</td>\n",
       "      <td>[ 1.29470171e-03  1.26969337e-03 -3.27199996e-...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>1154520</td>\n",
       "      <td>['ftp_##ftp.cs.rutgers.edu#pub#farach#SpliceJu...</td>\n",
       "      <td>that fits the asymptotics of the problem. Refe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1] D. Aldous and P. Shields. A diffusion limit...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "      <td>[-0.01303735  0.01778633  0.03181899  0.080227...</td>\n",
       "      <td>[ 4.66245227e-02 -1.68257095e-02 -7.54518732e-...</td>\n",
       "      <td>[-3.11294547e-03 -2.28649098e-02 -3.65703143e-...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>1154524</td>\n",
       "      <td>['http_##www.cs.rutgers.edu#~uli#cs671#ruttenb...</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper is a scientific comparison of two c...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Rule_Learning</td>\n",
       "      <td>[ 1.03734573e-02  8.40596855e-03  4.98973355e-...</td>\n",
       "      <td>[ 5.32725081e-02 -9.84915346e-03 -7.38246143e-...</td>\n",
       "      <td>[ 6.29070401e-03  3.94592695e-02 -5.06803021e-...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>1154525</td>\n",
       "      <td>['http_##www.cs.rutgers.edu#~uli#cs671#ertl-cc...</td>\n",
       "      <td>Instructions</td>\n",
       "      <td>M. Anton Ertl Andreas Krall</td>\n",
       "      <td>Paper and BibTeX entry are available at http:/...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Rule_Learning</td>\n",
       "      <td>[-4.73944703e-03  1.88433565e-02  1.70901790e-...</td>\n",
       "      <td>[ 7.37561807e-02 -2.18447298e-02 -7.84817040e-...</td>\n",
       "      <td>[ 4.75611426e-02 -3.42832354e-04 -1.49267381e-...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>1155073</td>\n",
       "      <td>['http_##l2r.cs.uiuc.edu#~danr#Other-papers#Pe...</td>\n",
       "      <td>Learning with Abduction</td>\n",
       "      <td>A.C. Kakas F. Riguzzi</td>\n",
       "      <td>We investigate how abduction and induction can...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Rule_Learning</td>\n",
       "      <td>[ 0.01221413  0.02293644  0.03390694  0.072650...</td>\n",
       "      <td>[ 6.93375543e-02 -1.24151595e-02 -7.49835223e-...</td>\n",
       "      <td>[ 1.78633258e-02  4.35449854e-02  1.89509813e-...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2368 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      paper_id                                          filenames  \\\n",
       "0           35  ['http_##www.cis.ohio-state.edu#lair#TechRepor...   \n",
       "1           40  ['http_##www.bioele.nuee.nagoya-u.ac.jp#wec#pa...   \n",
       "2          114  ['ftp_##ftp.cs.colorado.edu#users#baveja#Paper...   \n",
       "3          117  ['http_##www.cs.duke.edu#~mlittman#docs#gmdp.ps']   \n",
       "4          128  ['ftp_##ftp.cs.colorado.edu#users#baveja#Paper...   \n",
       "...        ...                                                ...   \n",
       "2363   1154276  ['ftp_##ftp.icsi.berkeley.edu#pub#techreports#...   \n",
       "2364   1154520  ['ftp_##ftp.cs.rutgers.edu#pub#farach#SpliceJu...   \n",
       "2365   1154524  ['http_##www.cs.rutgers.edu#~uli#cs671#ruttenb...   \n",
       "2366   1154525  ['http_##www.cs.rutgers.edu#~uli#cs671#ertl-cc...   \n",
       "2367   1155073  ['http_##l2r.cs.uiuc.edu#~danr#Other-papers#Pe...   \n",
       "\n",
       "                                                  Title  \\\n",
       "0     Evolutionary Module Acquisition (1993) Coevolv...   \n",
       "1     Dynamic Control of Genetic Algorithms using Fu...   \n",
       "2     Learning to Act using Real-Time Dynamic Progra...   \n",
       "3     Generalized Markov Decision Processes: Dynamic...   \n",
       "4     Reinforcement Learning Algorithms for Average-...   \n",
       "...                                                 ...   \n",
       "2363  Scatter-partitioning RBF network for function ...   \n",
       "2364  that fits the asymptotics of the problem. Refe...   \n",
       "2365                                           Abstract   \n",
       "2366                                       Instructions   \n",
       "2367                            Learning with Abduction   \n",
       "\n",
       "                                                 Author  \\\n",
       "0     Angeline and Pollack Angeline, P. and Pollack,...   \n",
       "1                        Michael A. LEE Hideyuki TAKAGI   \n",
       "2     Andrew G. Barto Steven J. Bradtke Satinder P. ...   \n",
       "3                   Csaba Szepesvari Michael L. Littman   \n",
       "4                                     Satinder P. Singh   \n",
       "...                                                 ...   \n",
       "2363                                                NaN   \n",
       "2364                                                NaN   \n",
       "2365                                                NaN   \n",
       "2366                        M. Anton Ertl Andreas Krall   \n",
       "2367                              A.C. Kakas F. Riguzzi   \n",
       "\n",
       "                                               Abstract  \\\n",
       "0     Angeline, P., Saunders, G. and Pollack, J. (19...   \n",
       "1     This paper proposes using fuzzy logic techniqu...   \n",
       "2     fl The authors thank Rich Yee, Vijay Gullapall...   \n",
       "3     The problem of maximizing the expected total d...   \n",
       "4     Reinforcement learning (RL) has become a centr...   \n",
       "...                                                 ...   \n",
       "2363  segmentation: Preliminary results Abstract. Sc...   \n",
       "2364  1] D. Aldous and P. Shields. A diffusion limit...   \n",
       "2365  This paper is a scientific comparison of two c...   \n",
       "2366  Paper and BibTeX entry are available at http:/...   \n",
       "2367  We investigate how abduction and induction can...   \n",
       "\n",
       "                                                    BoW  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2363  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2364  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2365  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2366  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2367  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                      topic2  \\\n",
       "0         Genetic_Algorithms   \n",
       "1         Genetic_Algorithms   \n",
       "2     Reinforcement_Learning   \n",
       "3     Reinforcement_Learning   \n",
       "4     Reinforcement_Learning   \n",
       "...                      ...   \n",
       "2363         Neural_Networks   \n",
       "2364         Neural_Networks   \n",
       "2365           Rule_Learning   \n",
       "2366           Rule_Learning   \n",
       "2367           Rule_Learning   \n",
       "\n",
       "                                           Abstract_W2V  \\\n",
       "0     [ 0.00257568  0.01624146 -0.01265259  0.129727...   \n",
       "1     [ 2.27050781e-02  4.92503271e-02  5.16006462e-...   \n",
       "2     [-0.01200994  0.03754203  0.00502043  0.093253...   \n",
       "3     [ 0.01002187  0.00597592  0.02532374  0.099285...   \n",
       "4     [ 0.01000858 -0.0008307   0.03161937  0.086836...   \n",
       "...                                                 ...   \n",
       "2363  [-0.00997262  0.04042348  0.02706561  0.063081...   \n",
       "2364  [-0.01303735  0.01778633  0.03181899  0.080227...   \n",
       "2365  [ 1.03734573e-02  8.40596855e-03  4.98973355e-...   \n",
       "2366  [-4.73944703e-03  1.88433565e-02  1.70901790e-...   \n",
       "2367  [ 0.01221413  0.02293644  0.03390694  0.072650...   \n",
       "\n",
       "                                         bert_embedding  \\\n",
       "0     [ 6.13429993e-02 -3.51591222e-02 -8.50764588e-...   \n",
       "1     [ 6.13822490e-02 -9.98590514e-03 -7.81871602e-...   \n",
       "2     [ 5.97590730e-02 -6.56006113e-03 -7.85881057e-...   \n",
       "3     [ 6.34309798e-02 -2.35317685e-02 -8.46546218e-...   \n",
       "4     [ 7.14411512e-02 -2.84189135e-02 -8.62762332e-...   \n",
       "...                                                 ...   \n",
       "2363  [ 4.68115099e-02 -3.41062658e-02 -8.05736557e-...   \n",
       "2364  [ 4.66245227e-02 -1.68257095e-02 -7.54518732e-...   \n",
       "2365  [ 5.32725081e-02 -9.84915346e-03 -7.38246143e-...   \n",
       "2366  [ 7.37561807e-02 -2.18447298e-02 -7.84817040e-...   \n",
       "2367  [ 6.93375543e-02 -1.24151595e-02 -7.49835223e-...   \n",
       "\n",
       "                                           cluster_embs  labels  \n",
       "0     [ 2.61604437e-03  2.69720778e-02 -3.40913646e-...       1  \n",
       "1     [ 1.28001701e-02  3.91861163e-02 -1.01053238e-...       1  \n",
       "2     [ 2.11583525e-02 -5.75699378e-03 -1.89218428e-...       4  \n",
       "3     [ 1.14918491e-02  3.67332175e-02  7.56769720e-...       4  \n",
       "4     [ 7.57048698e-03  3.90252322e-02  5.19991573e-...       4  \n",
       "...                                                 ...     ...  \n",
       "2363  [ 1.29470171e-03  1.26969337e-03 -3.27199996e-...       2  \n",
       "2364  [-3.11294547e-03 -2.28649098e-02 -3.65703143e-...       2  \n",
       "2365  [ 6.29070401e-03  3.94592695e-02 -5.06803021e-...       5  \n",
       "2366  [ 4.75611426e-02 -3.42832354e-04 -1.49267381e-...       5  \n",
       "2367  [ 1.78633258e-02  4.35449854e-02  1.89509813e-...       5  \n",
       "\n",
       "[2368 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d2db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_ft = cora_ft.train_test_split(test_size=0.2, seed=42)\n",
    "test_valid = cora_ft[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset_dict = {\n",
    "    \"train\": cora_ft[\"train\"],\n",
    "    \"validation\": test_valid[\"train\"],\n",
    "    \"test\": test_valid[\"test\"]\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09704d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 1894/1894 [00:03<00:00, 630.52 examples/s]\n",
      "Map: 100%|| 237/237 [00:00<00:00, 413.56 examples/s]\n",
      "Map: 100%|| 237/237 [00:00<00:00, 637.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def encode_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(encode_batch, batched=True)\n",
    "dataset = dataset.with_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febeba93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained(\"roberta-base\",num_labels=7)\n",
    "model = AutoAdapterModel.from_pretrained(\"roberta-base\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c508fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    }
   ],
   "source": [
    "n_labels = len(set(cora[\"topic2\"]))\n",
    "id2label = {i: lab for i, lab in enumerate(le.classes_)}\n",
    "# Add a new adapter\n",
    "model.add_adapter(\"cora_fn\", config=\"seq_bn\")\n",
    "\n",
    "# Add a matching classification head\n",
    "model.add_classification_head(\n",
    "    \"cora_fn\",\n",
    "    num_labels=n_labels,\n",
    "    id2label=id2label,\n",
    "  )\n",
    "\n",
    "# Activate the adapter\n",
    "model.train_adapter(\"cora_fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e62f5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=9,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=50,\n",
    "    output_dir=\"./cora_fn_out\",\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=[],\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  return {\"accuracy\": (preds == p.label_ids).mean()}\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7069e742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1071' max='1071' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1071/1071 2:00:22, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.882500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.843200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.356100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.978700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.936200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.643700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.607400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.595900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.522300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.520500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.505200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1071, training_loss=0.7617467208848058, metrics={'train_runtime': 7229.0951, 'train_samples_per_second': 2.358, 'train_steps_per_second': 0.148, 'total_flos': 1149150069080064.0, 'train_loss': 0.7617467208848058, 'epoch': 9.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "996a677e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5739740133285522,\n",
       " 'eval_accuracy': 0.8185654008438819,\n",
       " 'eval_runtime': 23.8145,\n",
       " 'eval_samples_per_second': 9.952,\n",
       " 'eval_steps_per_second': 0.336,\n",
       " 'epoch': 9.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa030ea4",
   "metadata": {},
   "source": [
    "## **Guardar y Leer nuevos embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380577bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active adapters: Stack[cora_fn]\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel\n",
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoAdapterModel.from_pretrained(\"roberta-base\")\n",
    "adapter_path = \"./cora_fn_out/checkpoint-1000/cora_fn\" # coger checkpoint para generar embeddings\n",
    "\n",
    "model.load_adapter(\n",
    "    adapter_path,\n",
    "    load_as=\"cora_fn\",\n",
    "    set_active=True \n",
    ")\n",
    "\n",
    "model.set_active_adapters(\"cora_fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def encode_abstracts(texts, batch_size=32, max_length=128):\n",
    "    all_embeddings = []\n",
    "\n",
    "    dataloader = DataLoader(texts, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = {k: v for k, v in inputs.items()}\n",
    "\n",
    "            # Forward pass through the (encoder only), ignoring the head\n",
    "            outputs = model.base_model(**inputs)  \n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "\n",
    "    return torch.cat(all_embeddings, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb60bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([2368, 768])\n"
     ]
    }
   ],
   "source": [
    "cora = pd.read_csv(\"../Hyperedges/Cora_dataset.csv\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "cora['labels'] = le.fit_transform(cora['topic2']) \n",
    "\n",
    "cora_ft = Dataset.from_pandas(cora[[\"Abstract\", \"labels\"]])\n",
    "cora_ft = cora_ft.rename_column(\"Abstract\", \"text\")\n",
    "\n",
    "# Coger los abstracts\n",
    "abstracts = cora_ft[\"text\"]\n",
    "\n",
    "# Compute embeddings\n",
    "embeddings = encode_abstracts(abstracts, batch_size=16)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed739922",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_np = embeddings.numpy()\n",
    "cora[\"cls_embedding\"] = list(embeddings_np)\n",
    "\n",
    "# Guardo el dataset con los nuevos embeddings\n",
    "cora.to_pickle(\"cora_with_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70534847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (19716, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.2475476 ,  0.6143033 ,  1.0025382 , ..., -0.05929684,\n",
       "        -0.02109905,  0.5139046 ],\n",
       "       [-0.04540623,  0.7384806 ,  0.7458422 , ...,  0.29003197,\n",
       "        -0.01394354,  0.41901493],\n",
       "       [-0.30693296,  0.4041639 ,  0.9072337 , ...,  0.17131016,\n",
       "        -0.04385674,  0.5062645 ],\n",
       "       ...,\n",
       "       [-0.04761544,  0.61029446,  0.85678405, ...,  0.01582604,\n",
       "         0.35217664,  0.4982269 ],\n",
       "       [-0.4546575 ,  0.3067275 ,  0.08663608, ..., -1.2792853 ,\n",
       "         0.34755304, -0.08672211],\n",
       "       [-0.19107838,  0.5639358 ,  0.3021389 , ..., -1.3055748 ,\n",
       "         0.23849142, -0.0692755 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# He hecho lo mismo fine-tuneando los embeddings de RoBERTa para los abstracts de PubMed\n",
    "# Load embeddings de PubMed\n",
    "\n",
    "loaded_embeddings = np.load(\"./PubMed_embeddings.npz\")[\"embeddings\"]\n",
    "print(\"Embeddings shape:\", loaded_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
