{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbbfff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq adapters datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6b177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.51.3\n",
      "Uninstalling transformers-4.51.3:\n",
      "  Successfully uninstalled transformers-4.51.3\n",
      "Found existing installation: adapters 1.2.0\n",
      "Uninstalling adapters-1.2.0:\n",
      "  Successfully uninstalled adapters-1.2.0\n",
      "Found existing installation: accelerate 0.26.0\n",
      "Uninstalling accelerate-0.26.0:\n",
      "  Successfully uninstalled accelerate-0.26.0\n",
      "Found existing installation: tokenizers 0.21.4\n",
      "Uninstalling tokenizers-0.21.4:\n",
      "  Successfully uninstalled tokenizers-0.21.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping peft as it is not installed.\n",
      "WARNING: Skipping trl as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.51.3\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.51.3)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.51.3) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\apala\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers==4.51.3) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers==4.51.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers==4.51.3) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers==4.51.3) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers==4.51.3) (2024.6.2)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   ---------------------------------------- 2/2 [transformers]\n",
      "\n",
      "Successfully installed tokenizers-0.21.4 transformers-4.51.3\n",
      "Collecting adapters==1.2.0\n",
      "  Using cached adapters-1.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: transformers~=4.51.3 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from adapters==1.2.0) (4.51.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from adapters==1.2.0) (23.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers~=4.51.3->adapters==1.2.0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers~=4.51.3->adapters==1.2.0) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers~=4.51.3->adapters==1.2.0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\apala\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers~=4.51.3->adapters==1.2.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers~=4.51.3->adapters==1.2.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers~=4.51.3->adapters==1.2.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers~=4.51.3->adapters==1.2.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers~=4.51.3->adapters==1.2.0) (2024.6.2)\n",
      "Using cached adapters-1.2.0-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: adapters\n",
      "Successfully installed adapters-1.2.0\n",
      "Collecting accelerate==0.26.0\n",
      "  Using cached accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate==0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate==0.26.0) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\apala\\appdata\\roaming\\python\\python312\\site-packages (from accelerate==0.26.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate==0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate==0.26.0) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate==0.26.0) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate==0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (75.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub->accelerate==0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub->accelerate==0.26.0) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\apala\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate==0.26.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\apala\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate==0.26.0) (1.3.0)\n",
      "Using cached accelerate-0.26.0-py3-none-any.whl (270 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y transformers adapters peft trl accelerate tokenizers\n",
    "!pip install transformers==4.51.3\n",
    "!pip install adapters==1.2.0\n",
    "!pip install accelerate==0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7416e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\apala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import RobertaTokenizer, RobertaConfig, TrainingArguments, EvalPrediction\n",
    "from adapters import AutoAdapterModel, AdapterTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea47d76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 8530, 'validation': 1066, 'test': 1066}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tom = load_dataset(\"rotten_tomatoes\")\n",
    "data_tom.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80755c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n",
      "{'text': 'effective but too-tepid biopic', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(data_tom)\n",
    "print(data_tom[\"train\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad8f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = pd.read_csv(\"../Hyperedges/Cora_dataset.csv\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "cora['labels'] = le.fit_transform(cora['topic2']) \n",
    "\n",
    "cora_ft = Dataset.from_pandas(cora[[\"Abstract\", \"labels\"]])\n",
    "cora_ft = cora_ft.rename_column(\"Abstract\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d2db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_ft = cora_ft.train_test_split(test_size=0.2, seed=42)\n",
    "test_valid = cora_ft[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset_dict = {\n",
    "    \"train\": cora_ft[\"train\"],\n",
    "    \"validation\": test_valid[\"train\"],\n",
    "    \"test\": test_valid[\"test\"]\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09704d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1894/1894 [00:03<00:00, 630.52 examples/s]\n",
      "Map: 100%|██████████| 237/237 [00:00<00:00, 413.56 examples/s]\n",
      "Map: 100%|██████████| 237/237 [00:00<00:00, 637.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def encode_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(encode_batch, batched=True)\n",
    "dataset = dataset.with_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febeba93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained(\"roberta-base\",num_labels=7)\n",
    "model = AutoAdapterModel.from_pretrained(\"roberta-base\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c508fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    }
   ],
   "source": [
    "n_labels = len(set(cora[\"topic2\"]))\n",
    "id2label = {i: lab for i, lab in enumerate(le.classes_)}\n",
    "# Add a new adapter\n",
    "model.add_adapter(\"cora_fn\", config=\"seq_bn\")\n",
    "\n",
    "# Add a matching classification head\n",
    "model.add_classification_head(\n",
    "    \"cora_fn\",\n",
    "    num_labels=n_labels,\n",
    "    id2label=id2label,\n",
    "  )\n",
    "\n",
    "# Activate the adapter\n",
    "model.train_adapter(\"cora_fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e62f5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=9,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=50,\n",
    "    output_dir=\"./cora_fn_out\",\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=[],\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  return {\"accuracy\": (preds == p.label_ids).mean()}\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7069e742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1071' max='1071' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1071/1071 2:00:22, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.882500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.843200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.356100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.978700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.936200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.643700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.607400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.595900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.522300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.520500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.505200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1071, training_loss=0.7617467208848058, metrics={'train_runtime': 7229.0951, 'train_samples_per_second': 2.358, 'train_steps_per_second': 0.148, 'total_flos': 1149150069080064.0, 'train_loss': 0.7617467208848058, 'epoch': 9.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "996a677e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5739740133285522,\n",
       " 'eval_accuracy': 0.8185654008438819,\n",
       " 'eval_runtime': 23.8145,\n",
       " 'eval_samples_per_second': 9.952,\n",
       " 'eval_steps_per_second': 0.336,\n",
       " 'epoch': 9.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
