{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59cb61dd",
   "metadata": {},
   "source": [
    "## **Making Embeddings for PubMed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922581c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b97270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = pd.read_csv(\"../PubMed_dataset.csv\")\n",
    "\n",
    "# Load pretrained Word2Vec model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd15a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PMID                                       Abstract_W2V\n",
      "0  12187484  [-0.03999741, 0.062728785, -0.002549661, 0.049...\n",
      "1   2344352  [0.014284923, 0.03845855, 0.020136734, 0.05924...\n",
      "2  14654069  [-0.020149924, 0.047227394, 0.012617389, 0.035...\n",
      "3  16443886  [-0.045007102, 0.025433676, -0.005037438, 0.06...\n",
      "4   2684155  [-0.028360292, 0.05821858, 0.015442131, 0.0421...\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec embedding\n",
    "\n",
    "# Get embedding for a single abstract\n",
    "def abstract_to_w2v(abstract, w2v_model, vector_size=300):\n",
    "    # Tokenize by simple whitespace and lowercase\n",
    "    tokens = abstract.lower().split()\n",
    "    # Filter tokens that exist in the Word2Vec vocabulary\n",
    "    valid_tokens = [t for t in tokens if t in w2v_model]\n",
    "    \n",
    "    if not valid_tokens:  # If no tokens are in the vocab, return zeros\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    # Average embeddings\n",
    "    embeddings = np.array([w2v_model[t] for t in valid_tokens])\n",
    "    return embeddings.mean(axis=0)\n",
    "\n",
    "# Apply to the dataset\n",
    "vector_size = 300  \n",
    "dataset[\"Abstract_W2V\"] = dataset[\"Abstract\"].apply(lambda x: abstract_to_w2v(x, model, vector_size))\n",
    "\n",
    "# Check the result\n",
    "print(dataset[[\"PMID\", \"Abstract_W2V\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92625ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19716/19716 [8:14:00<00:00,  1.50s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (19716, 768)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_FOLDER = \"./daberta_finetune_head_PubMed\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_FOLDER)\n",
    "model = AutoModel.from_pretrained(MODEL_FOLDER)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  \n",
    "\n",
    "# Function to get embeddings\n",
    "def get_bert_embedding(text, tokenizer, model, device, max_length=512):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  # shape: (1, seq_len, hidden_size)\n",
    "        cls_embedding = last_hidden_state[:, 0, :]      # CLS token embedding\n",
    "        return cls_embedding.squeeze().cpu().numpy()\n",
    "\n",
    "# Compute embeddings for all abstracts\n",
    "embeddings = []\n",
    "\n",
    "for abstract in tqdm(dataset['Abstract']):\n",
    "    emb = get_bert_embedding(abstract, tokenizer, model, device)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(\"Embeddings shape:\", embeddings.shape)  # (num_abstracts, hidden_size)\n",
    "\n",
    "# Add embeddings to the DataFrame\n",
    "dataset['bert_embedding'] = list(embeddings)\n",
    "\n",
    "dataset.to_csv(\"PubMed_dataset_EMBS.csv\", index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
